<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Reasoning LLMs Are Just Efficient Samplers:
        RL Training Elicits No Transcending Capacity"> 
  <meta name="keywords" content="Qwen, Deepseek-R1, PPO, GRPO, AIME, RLVR, Tsinghua University"> <!-- TODO: add some keywords for search engine -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome_6_7_2.all.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome_6_7_2.all.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning
          </h1>

          <div class="publication-authors">
              <!-- 作者列表 -->
            <div class="is-size-4 author-list">
              <span class="author-block">
                <a href="https://shenzhi-wang.netlify.app/" class="author-name">Shenzhi Wang</a><sup class="affiliation">1,2</sup>
              </span>
              <span class="author-block">
                <a href="https://yule-buaa.github.io/" class="author-name">Le Yu</a><sup class="affiliation">1</sup>
              </span>
              <span class="author-block">
                <a href="https://gao-xiao-bai.github.io/" class="author-name">Gao Chang</a><sup class="affiliation">1</sup>
              </span>
              <span class="author-block">
                <a href="https://chujiezheng.github.io/" class="author-name">Chujie Zheng</a><sup class="affiliation">1</sup>
              </span>
              <span class="author-block">
                <a href="" class="author-name">Shixuan Liu</a><sup class="affiliation">1</sup>
              </span>
              <span class="author-block">
                <a href="https://lr32768.github.io/" class="author-name">Rui Lu</a><sup class="affiliation">2</sup>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=WQuf6hwAAAAJ" class="author-name">Kai Dang</a><sup class="affiliation">1</sup>
              </span>
              <span class="author-block">
                <a href="https://xionghuichen.github.io/" class="author-name">Xionghui Chen</a><sup class="affiliation">1</sup>
              </span>
              <span class="author-block">
                <a href="" class="author-name">Jianxin Yang</a><sup class="affiliation">1</sup>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=scujDKkAAAAJ" class="author-name">Zhenru Zhang</a><sup class="affiliation">1</sup>
              </span>
              <span class="author-block">
                <a href="" class="author-name">Yuqiong Liu</a><sup class="affiliation">1</sup>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=vO9FZekAAAAJ" class="author-name">An Yang</a><sup class="affiliation">1</sup>
              </span>
              <span class="author-block">
                <a href="https://andrewzh112.github.io/" class="author-name">Andrew Zhao</a><sup class="affiliation">2</sup>
              </span>
              <span class="author-block">
                <a href="https://yueyang130.github.io/" class="author-name">Yang Yue</a><sup class="affiliation">2</sup>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=rw6vWdcAAAAJ" class="author-name">Shiji Song</a><sup class="affiliation">2</sup>
              </span>
              <span class="author-block">
                <a href="https://yubowen-ph.github.io/" class="author-name">Bowen Yu</a><sup class="affiliation">1,&#9993;,†</sup>
              </span>
              <span class="author-block">
                <a href="https://www.gaohuang.net/" class="author-name">Gao Huang</a><sup class="affiliation">2,&#9993;</sup>
              </span>
              <span class="author-block">
                and <a href="https://justinlin610.github.io/" class="author-name">Junyang Lin</a><sup class="affiliation">1</sup>
              </span>
            </div>
          
            <!-- 机构信息 -->
            <div class="institutions is-size-5">
              <span class="institution"><sup>1</sup><a href="https://huggingface.co/Qwen">Qwen Team, Alibaba Inc.</a></span>
              <span class="institution">&nbsp;&nbsp;</span> <!-- 间隔符 -->
              <span class="institution"><sup>2</sup><a href="https://www.leaplab.ai">LeapLab, Tsinghua University</a></span>
            </div>
          
            <!-- 贡献说明 -->
            <div class="contribution-notes is-size-5">
              <span class="note"><sup>&#9993;</sup> Corresponding Authors</span>
              <span class="note">&nbsp;&nbsp;</span>
              <span class="note"><sup>†</sup>Project Lead</span>
            </div>
          
            <div class="corresponding-authors is-size-5">
              Emails: 
              <a href="mailto:wangshenzhi99@gmail.com">
                wangshenzhi99@gmail.com
              </a>, 
              <a href="mailto:yubowen.ph@gmail.com">
                yubowen.ph@gmail.com
              </a>,
              <a href="mailto:gaohuang@tsinghua.edu.cn">
                gaohuang@tsinghua.edu.cn
              </a>
            </div>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2506.01939"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="fas fa-file-pdf"></i> -->
                      <i class="fa-solid fa-file-pdf" style="color: #ec4646;"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Arxiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2506.01939"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub (Comming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-regular fa-database"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
              <!-- Model Link. -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/BytedTsinghua-SIA/DAPO-Qwen-32B"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"> -->
                    <!-- <i class="fa-solid fa-face-smiling-hands"></i> -->
                    <!-- <i class="fa-solid fa-face-smiling-hands" style="color: #FFD43B;"></i>
                  </span>
                  <span>Model</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Video -->
    <div class="columns is-centered">
      <div class="column">
        <h2 class="subtitle has-text-centered">
          <img src="./static/images/teaser_png.png" style="display: block; margin: 0 auto; width: 80%;"/>
        </h2>
        <p style="font-size: 0.8rem;">
          (a) In CoTs, only a minority of tokens exhibit high entropy and act as  "forks" in reasoning paths, while majority tokens are low-entropy. (b) RLVR using policy gradients of forking tokens delivers significant performance gains that scale with model size. With a 20k maximum response length, our 32B model sets new SoTA scores (63.5 on AIME'24 and 56.7 on AIME'25) for RLVR on base models under 600B. Extending the maximum response length to 29k further boosts the AIME'24 score to 68.1.
        </p>
      </div>
    </div>
  </div>
<!-- </section>    

<section class="section"> -->

  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-four-fifths"> -->
          <div class="content has-text-justified">
            <p>  
              <br>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake <b>a pioneering exploration of RLVR through the novel perspective of token entropy patterns</b>, comprehensively analyzing how different tokens influence reasoning performance. 
            </p>  
          </div>
      <!-- </div> -->
    </div>
   
        <!-- <section class="section"> -->
          <div class="columns is-centered has-text-centered">
            <h2 class="title is-3"><br>Takeaways</h2>
          </div>
          <div class="container is-max-desktop">
          <!-- Q&A Section -->
          <div class="columns is-centered">
            <!-- <div class="qa-answer">
              <div class="a-marker">
                <span class="a-icon">1</span>
              </div>
              <div class="answer-text" style="font-size: 1em;">
                <p>
                  In CoTs, the majority of tokens are generated with low entropy, while only a small subset exhibits high entropy. These high-entropy minority tokens often act as "forks" in the reasoning process, guiding the model toward diverse reasoning paths. Maintaining high entropy at these critical forking tokens is beneficial for reasoning performance.
                </p>
              </div>
            </div> -->

            <div class="qa-answer" style="flex-direction: column; gap: 1rem;">
              <div class="qa-row" style="display: flex; align-items: flex-start; gap: 0.5rem;">
                <div class="a-marker">
                  <span class="a-icon">1</span>
                </div>
                <div class="answer-text" style="font-size: 1em;">
                  <p>
                    <b>Entropy patterns in CoTs.</b> In CoTs, the majority of tokens are generated with low entropy, while only a small subset exhibits high entropy. These high-entropy minority tokens often act as "forks" in the reasoning process, guiding the model toward diverse reasoning paths. Maintaining high entropy at these critical forking tokens is beneficial for reasoning performance. 
                  </p>
                </div>
              </div>

              <div class="qa-row" style="display: flex; align-items: flex-start; gap: 0.5rem;">
                <div class="a-marker">
                  <span class="a-icon">2</span>
                </div>
                <div class="answer-text" style="font-size: 1em;">
                  <p>
                    <b>Evolution of entropy patterns in CoTs during RLVR.</b> During RLVR training, the reasoning model largely preserves the base model's entropy patterns, showing only gradual and minor changes. RLVR primarily adjusts the entropy of high-entropy tokens, while the entropy of low-entropy tokens fluctuates only within a narrow range.
                  </p>
                </div>
              </div>

              <div class="qa-row" style="display: flex; align-items: flex-start; gap: 0.5rem;">
                <div class="a-marker">
                  <span class="a-icon">3</span>
                </div>
                <div class="answer-text" style="font-size: 1em;">
                  <p>
                    <b>High-entropy minority tokens drive nearly all reasoning performance gains during RLVR</b>, whereas low-entropy majority tokens contribute little or may even hinder performance. One possible explanation is that, prior to performance convergence, a subset (~20% in our experiments) of high-entropy tokens facilitates exploration, while low-entropy tokens offer minimal benefit or may even impede it.
                  </p>
                </div>
              </div>

              <div class="qa-row" style="display: flex; align-items: flex-start; gap: 0.5rem;">
                <div class="a-marker">
                  <span class="a-icon">4</span>
                </div>
                <div class="answer-text" style="font-size: 1em;">
                  <p>
                    <b>More discussions and insights.</b> Based on the insights above, we further discuss (i) high-entropy minority tokens as a potential reason why supervised fine-tuning (SFT) memorizes but RL generalizes, (ii) how prior knowledge and readability requirements shape the different entropy patterns seen in LLM CoTs compared to traditional RL trajectories, and (iii) the advantage of clip-higher over entropy bonus for RLVR.
                  </p>
                </div>
              </div>
            </div>

            <!-- <div class="qa-answer">
              <div class="a-marker">
                <span class="a-icon">3</span>
              </div>
              <div class="answer-text" style="font-size: 1em;">
                <p>
                  High-entropy minority tokens drive nearly all reasoning performance gains during RLVR, whereas low-entropy majority tokens contribute little or may even hinder performance. One possible explanation is that, prior to performance convergence, a subset ($\sim20\%$ in our experiments) of high-entropy tokens facilitates exploration, while low-entropy tokens offer minimal benefit or may even impede it.
                </p>
              </div>
            </div>

            <div class="qa-answer">
              <div class="a-marker">
                <span class="a-icon">4</span>
              </div>
              <div class="answer-text" style="font-size: 1em;">
                <p>
                  Based on the insights above, we further discuss (i) high-entropy minority tokens as a potential reason why supervised fine-tuning (SFT) memorizes but RL generalizes, (ii) how prior knowledge and readability requirements shape the different entropy patterns seen in LLM CoTs compared to traditional RL trajectories, and (iii) the advantage of clip-higher over entropy bonus for RLVR.
                </p>
              </div>
            </div> -->

          </div>
    </div>
  </div>
<!-- </section> -->


<!-- <section class="section"> -->
  <div class="container is-max-desktop">
    <!-- Conclusion -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3"><br>Entropy Patterns in CoTs</h2>
        <div class="content has-text-justified">
          <ol>
            <li>
            <span style="font-size: 1.15em;"><b>High-entropy tokens are the minority, while low-entropy tokens constitute the majority.</b></span> <br>
            Typically, only a minority of tokens are generated with high entropy, while a majority of tokens are outputted with low entropy.
            </li>
            <li>
            <span style="font-size: 1.15em;"><b>Highest-entropy tokens serve to bridge the logic, while lowest-entropy tokens completes it.</b></span> <br>
            Tokens with the highest entropy typically serve to bridge the logical connection between two consecutive parts of reasoning, while tokens with the lowest entropy tend to complete the current part of a sentence or finish constructing a word. Other tokens combine these two functions to varying degrees.
            </li>
            <img src="./static/images/cot_entropy.png" style="display: block; margin: 0 auto; width: 80%;"/>
            <p style="font-size: 0.8rem;">
              Entropy patterns in the chain of thoughts of LLMs. <b>(a) Token entropy distribution.</b> The Y-axis frequency is on a <i>log scale</i>. A minority of tokens exhibit high entropy, while the majority have low entropy, often approaching zero. <b>(b) & (c) Word clouds of the top 100 tokens with the highest and lowest average entropy, respectively, selected from the set of frequently occurring tokens.</b> A larger font size indicates a higher average token entropy. Tokens with the highest average entropy typically function as "forks" to determine reasoning directions, whereas tokens with the lowest average entropy tend to execute reasoning steps along the established path.
            </p>

            <li>
            <span style="font-size: 1.15em;"><b>High-entropy tokens as "forks" in CoTs.</b></span> <br>
            High-entropy tokens benefit from being assigned a relatively higher temperature compared to other tokens. Given that high-entropy tokens naturally exhibit higher entropy than other tokens, this further supports the need for them to operate at an even higher entropy level. <b>This observation indicates their role as "forks," where high entropy enables them to branch into diverse reasoning directions.</b> Therefore, we also refer to these high-entropy tokens as <b>forking tokens</b>.
            </li>
            <img src="./static/images/inference_with_varying_temp.png" style="display: block; margin: 0 auto; width: 50%;"/>
            <p style="font-size: 0.8rem;">
              Average scores of AIME 2024 and AIME 2025. <b>Red curve:</b> varying the decoding temperature of high-entropy tokens while keeping the decoding temperature of low-entropy tokens fixed at 1. <b>Blue curve:</b> adjusting the decoding temperature of low-entropy tokens while maintaining the decoding temperature of high-entropy tokens at 1.
            </p>
            </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
<!-- </section> -->


<!-- <section class="section"> -->
  <div class="container is-max-desktop">
    <!-- Conclusion -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3"><br>RLVR Preserves and Reinforces Base Model Entropy Patterns</h2>

        <div class="content has-text-justified">
          <ol>
            <li>
              <span style="font-size: 1.15em;"><b>RLVR primarily preserves the existing entropy patterns of the base models.</b></span> <br>
              As shown in the following table, although overlap with the base model gradually decreases and overlap with the final RLVR model increases, the base model's overlap still remains above 86% at convergence (step 1360), suggesting that RLVR largely retains the base model's entropy patterns regarding which tokens exhibit high or low uncertainty.
            </li>

<table style="width:100%; border-collapse:collapse; font-size:small;">
  <p style="font-size: 0.8rem;">
    <br>The progression of the overlap ratio in the positions of the top 20% high-entropy tokens, comparing the base model (i.e., step 0) with the model after RLVR training (i.e., step 1360).
  </p>
  <thead style="border-bottom:2px solid #000;">
    <tr>
      <th style="border:1px solid #ddd; padding:5px; text-align:left;">Compared w/</th>
      <th style="border:1px solid #ddd; padding:5px; text-align:center;">Step 0</th>
      <th style="border:1px solid #ddd; padding:5px; text-align:center;">Step 16</th>
      <th style="border:1px solid #ddd; padding:5px; text-align:center;">Step 112</th>
      <th style="border:1px solid #ddd; padding:5px; text-align:center;">Step 160</th>
      <th style="border:1px solid #ddd; padding:5px; text-align:center;">Step 480</th>
      <th style="border:1px solid #ddd; padding:5px; text-align:center;">Step 800</th>
      <th style="border:1px solid #ddd; padding:5px; text-align:center;">Step 864</th>
      <th style="border:1px solid #ddd; padding:5px; text-align:center;">Step 840</th>
      <th style="border:1px solid #ddd; padding:5px; text-align:center;">Step 1280</th>
      <th style="border:1px solid #ddd; padding:5px; text-align:center;">Step 1360</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border:1px solid #ddd; padding:5px;">Base Model</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">100%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">98.92%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">98.70%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">93.04%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">93.02%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">93.03%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">87.45%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">87.22%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">87.09%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">86.67%</td>
    </tr>
    <tr style="border-bottom:2px solid #000;">
      <td style="border:1px solid #ddd; padding:5px;">RLVR Model</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">86.67%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">86.71%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">86.83%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">90.64%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">90.65%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">90.64%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">96.61%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">97.07%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">97.34%</td>
      <td style="border:1px solid #ddd; padding:5px; text-align:center;">100%</td>
    </tr>
  </tbody>
</table>

          <li>
            <span style="font-size: 1.15em;"><b>RLVR predominantly alters the entropy of high-entropy tokens, whereas the entropy of low-entropy tokens remains comparatively stable with minimal variations.</b></span> <br>
               We compute the average entropy change after RLVR for each 5% entropy percentile range of the base model in the following figure. It is observed that tokens with higher initial entropy in the base model tend to exhibit larger increases in entropy after RLVR. This observation could also further reinforce that RLVR primarily preserves the entropy patterns of the base model.
            <img src="./static/images/entropy_change_percentiles.png" style="display: block; margin: 0 auto; width: 80%;"/>
            <p style="font-size: 0.8rem;">
              Average entropy change after RLVR within each 5% entropy percentile range of the base model. x% percentile means that x% of the tokens in the dataset have entropy values less than or equal to this value. It is worth noting that the Y-axis is presented on a <i>log scale</i>. Tokens with higher initial entropy tend to experience greater entropy increases after RLVR.
            </p>
          </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
<!-- </section> -->




<!-- <section class="section"> -->
  <div class="container is-max-desktop">
    <!-- Conclusion -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3"><br>High-Entropy Minority Tokens Drive Effective RLVR</h2>

        <div class="content has-text-justified">
          <ol>
            We use <a href="https://arxiv.org/abs/2503.14476">DAPO</a> as the baseline and adapt it by <b>discarding the policy gradients of the bottom 80% low-entropy tokens, focusing instead on training the model using only the policy gradients of the top 20% high-entropy tokens (i.e., forking tokens).</b><br><br>
            <li>
              Retaining only the top 20% of high-entropy tokens does not negatively impact performance on the 8B model and even leads to significant improvements on the 14B and 32B models, highlighting <b>a strong scaling trend as model size increases</b>.
              <img src="./static/images/rlvr_performance_table.png" style="display: block; margin: 0 auto; width: 80%;"/>
              <p style="font-size: 0.8rem;">
                Comparison between <i>vanilla DAPO using all tokens</i> and <i>DAPO using only the top 20% high-entropy tokens (i.e. forking tokens)</i> in policy gradient loss, evaluated on the <i>Qwen3-32B</i>, <i>Qwen3-14B</i> and <i>Qwen3-8B</i> base models. "Acc@16" and "Len@16" denotes the average accuracy and response length over 16 evaluations per benchmark, respectively.<br>
              </p>
            </li>

            <li>
              Especially, with a 20k maximum response length, <b>our 32B model sets new SoTA scores (63.5 on AIME'24 and 56.7 on AIME'25) for RLVR on base models under 600B</b>.
              <img src="./static/images/rlvr_32b.png" style="display: block; margin: 0 auto; width: 80%;"/>
              <p style="font-size: 0.8rem;">
                Curves of AIME'24 scores and response lengths with a maximum response length of 20k, trained from the Qwen3-32B base model. Dropping the bottom 80% low-entropy tokens stabilizes training and improves the AIME'24 score by 7.73.<br>
              </p>
            </li>

            
            <li>
              Extending the maximum response length to 29k <b>further boosts the AIME'24 score to 68.1</b>.
              <img src="./static/images/longer_context.png" style="display: block; margin: 0 auto; width: 80%;"/>
              <p style="font-size: 0.8rem;">
                By extending the maximum response length from 20k to 29k and continuing training from the SoTA 32B model shown in the figure above, the AIME'24 scores improve further from 63.54 to 68.12, alongside a notable increase in response length.
              </p>
            </li>

          </ol>
        </div>
      </div>
    </div>
  </div>
<!-- </section> -->

  <div class="container is-max-desktop">
    <!-- Conclusion -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3"><br>Discussions</h2>

        <div class="content has-text-justified">
          <ol>
            
            <li>
              <b>High-entropy minority tokens (i.e., forking tokens) could play a key role in explaining why RL generalizes while SFT memorizes.</b>
            </li>

            <li>
              <b>Unlike traditional RL, LLM reasoning integrates prior knowledge and must produce readable output. Consequently, LLM CoTs contain a mix of low-entropy majority tokens and high-entropy minority tokens, whereas traditional RL can assume uniform action entropy throughout a trajectory.</b>
            </li>

            
            <li>
              <b>In RLVR, entropy bonus may be suboptimal, as it increases the entropy of low-entropy majority tokens. In contrast, clip-higher effectively promotes entropy in high-entropy minority tokens.</b>
            </li>

          </ol>
        </div>
      </div>
    </div>
  </div>

<style>
  .qa-card {
    background: linear-gradient(145deg, #f8f9fa 0%, #ffffff 100%);
    border-radius: 15px;
    box-shadow: 0 4px 20px rgba(0,0,0,0.08);
    margin: 2.5rem 0;
    padding: 2.5rem;
    transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
    position: relative;
    overflow: hidden;
  }
  
  .qa-card:hover {
    transform: translateY(-5px);
    box-shadow: 0 8px 30px rgba(255,0,0,0.15); /* 红色 */
  }
  
  .qa-card:before {
    content: "";
    position: absolute;
    left: 0;
    top: 0;
    height: 100%;
    width: 4px;
    background: linear-gradient(180deg, #FF0000 0%, #308030 100%); /* 红到绿 */
  }
  
  .q-marker {
    display: flex;
    align-items: center;
    gap: 1.5rem;
    margin-bottom: 1.5rem;
  }
  
  .q-number {
    font-size: 1.4rem;
    font-weight: 800;
    color: #FF0000; /* 红色 */
    min-width: 50px;
    position: relative;
  }
  
  .q-number:after {
    content: "";
    position: absolute;
    right: -15px;
    top: 50%;
    transform: translateY(-50%);
    width: 6px;
    height: 6px;
    background: #308030;
    border-radius: 50%;
  }
  
  .q-icon, .a-icon {
    font-size: 1.8rem;
    font-weight: 800;
    width: 45px;
    height: 45px;
    border-radius: 12px;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
  }
  
  .q-icon {
    background: linear-gradient(135deg, #FF0000 0%, #CC0000 100%); /* 红色渐变 */
    color: white;
  }
  
  .a-icon {
    background: linear-gradient(135deg, #308030 0%, #1f6110 100%);
    color: white;
  }
  
  .qa-question {
    display: flex;
    align-items: flex-start;
  }
  
  .question-text {
    font-size: 1.3rem;
    color: #840b0b;
    margin: 0;
    line-height: 1.5;
    position: relative;
    padding-left: 2rem;
  }
  
  .question-text:before {
    content: "?";
    position: absolute;
    left: 0;
    top: -0.2em;
    font-size: 1.8em;
    color: #FF0000; /* 红色 */
    opacity: 0.2;
    font-weight: 800;
  }
  
  .qa-answer {
    display: flex;
    gap: 1.5rem;
    margin-top: 2rem;
    padding: 1.5rem;
    background: rgba(76,175,80,0.05);
    border-radius: 12px;
    position: relative;
    margin-left: 0rem;
  }
  
  .answer-text {
    font-size: 1.1rem;
    line-height: 1.8;
    color: #37474f;
    position: relative;
    padding-left: 2rem;
  }
  
  .answer-text:before {
    content: "➤";
    position: absolute;
    left: 0;
    color: #308030;
    font-size: 1.2em;
    top: 0.1em;
  }

  @media (max-width: 480px) {
    .qa-card {
      padding: 1.2rem;
      margin: 1.5rem 0;
      border-radius: 12px;
    }

    .q-marker {
      gap: 1rem;
      margin-bottom: 1rem;
    }

    .q-number {
      font-size: 1.5rem !important;
      min-width: 40px;
    }

    .q-icon, .a-icon {
      width: 36px;
      height: 36px;
      font-size: 1.4rem;
      border-radius: 8px;
    }

    .question-text {
      font-size: 1.15rem;
      line-height: 1.4;
      padding-left: 1.5rem;
    }

    .qa-answer {
      margin: 1.2rem 0 0 0;
      padding: 1rem;
      border-radius: 10px;
      gap: 1rem;
    }

    .answer-text {
      font-size: 1rem;
      line-height: 1.6;
      padding-left: 1.5rem;
    }

    .answer-text:before {
      left: -0.2rem;
    }

    .qa-card:before {
      width: 3px;
    }

    p {
      margin-bottom: 0.8em !important;
    }

    .title.is-3 {
      font-size: 1.5rem !important;
      margin-bottom: 2rem !important;
    }
  }

  @media (max-width: 768px) {
    .qa-card {
      padding: 1.5rem;
    }
    @media (max-width: 480px) {
      .qa-card {
        padding: 1.2rem;
      }
    }
  }
  
  /* @media (max-width: 768px) {
    .qa-card {
      padding: 1.5rem;
      margin: 1.5rem 0;
    }
    .qa-answer {
      margin-left: 0;
    }
    .question-text {
      padding-left: 0;
    }
    .question-text:before {
      display: none;
    }
  } */
</style>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Fully Open-Source -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">BibTeX</h2>
        <div class="content has-text-justified">
          <pre><code>@misc{wang20258020rulehighentropyminority,
      title={Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning}, 
      author={Shenzhi Wang and Le Yu and Chang Gao and Chujie Zheng and Shixuan Liu and Rui Lu and Kai Dang and Xionghui Chen and Jianxin Yang and Zhenru Zhang and Yuqiong Liu and An Yang and Andrew Zhao and Yang Yue and Shiji Song and Bowen Yu and Gao Huang and Junyang Lin},
      year={2025},
      eprint={2506.01939},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.01939}, 
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>


</body>
</html>
